{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "g5uuLz-FS3_B",
        "QkLjkwpDFCdF",
        "JVMPNOikqFJS",
        "htXPiMwHqU0x",
        "JXy7MjujTN7-",
        "TuxfFuY-efPG",
        "gzZHndL8baFZ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gabriela-Alcaide/Aprendizado_por_Reforco_IC/blob/main/Bike_Workers_%2B_GUROBI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classe do Ambiente"
      ],
      "metadata": {
        "id": "g5uuLz-FS3_B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "MqmXZoReNPIQ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from itertools import product, chain\n",
        "import numpy as np\n",
        "\n",
        "class BikeWorkers_Environment:\n",
        "    def __init__ (self, n_W, n_S):\n",
        "        self.n_W = n_W\n",
        "        self.n_S = n_S\n",
        "\n",
        "        self.state = tuple([0]*(self.n_W+1))\n",
        "\n",
        "        self.states = list(product(range(n_S+1), repeat=n_W+1))\n",
        "        self.actions = tuple(x for x in range(n_W+1))\n",
        "\n",
        "        self.Pcw = 0.8\n",
        "        self.Pcx = np.random.rand(n_S)\n",
        "        self.Pcx = (1-self.Pcw)*self.Pcx/self.Pcx.sum()\n",
        "\n",
        "        self.Pwxy = np.random.rand(n_W,n_S)\n",
        "\n",
        "        self.Gx = .5 + np.random.rand(n_S)\n",
        "        self.Hxy = np.random.rand(n_W,n_S)\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = tuple([0]*(self.n_W+1))\n",
        "\n",
        "    def nextState(self,state, action):\n",
        "        nextStates = {}\n",
        "\n",
        "        # Lista de possíveis valores para o próximo estado.\n",
        "        cart_dim = []\n",
        "        # Probabilidade de cada possível valor.\n",
        "        cart_dim_prob = []\n",
        "\n",
        "        # Lista de todos os serviços.\n",
        "        Cs = list(range(self.n_S+1))\n",
        "        # Concatena as probabilidades.\n",
        "        PCs = np.concatenate(([self.Pcw], self.Pcx))\n",
        "\n",
        "        # Atualiza o conjunto de possibilidades.\n",
        "        cart_dim = cart_dim + [Cs]\n",
        "        cart_dim_prob = cart_dim_prob + [PCs]\n",
        "\n",
        "        # Para cada trabalhadora.\n",
        "        for w in range(self.n_W):\n",
        "          # Se está \"waiting\":\n",
        "          if state[w+1] == 0:  # Tudo é determinista.\n",
        "            PWs = [1]\n",
        "            # Se a ação foi alocar um serviço para essa entregadora, ela o recebe.\n",
        "            if action == w+1:\n",
        "              Ws = [state[0]]   # Se a trabalhadora foi escolhida, aloca o serviço atual\n",
        "            # Se não, continua \"waiting\".\n",
        "            else:\n",
        "              Ws = [0]\n",
        "          # Se já estava ocupada.\n",
        "          else:\n",
        "            # Pode entregar o serviço e ir para \"waiting\" ou continuar trabalhando no serviço atual.\n",
        "            Ws = [0, state[w+1]]\n",
        "            # A probabilidade de entregar o serviço e a de não entregá-lo (continuar nele).\n",
        "            PWs = [self.Pwxy[w][state[w+1]-1] , 1 - self.Pwxy[w][state[w+1]-1] ]\n",
        "\n",
        "          # Atualiza.\n",
        "          cart_dim = cart_dim + [Ws]\n",
        "          cart_dim_prob = cart_dim_prob + [PWs]\n",
        "\n",
        "        # Todos os estados futuros possíveis, e suas probabilidades.\n",
        "        states = list(product(*cart_dim))\n",
        "        probs = list(product(*cart_dim_prob))\n",
        "\n",
        "        # Coloca no dicionário.\n",
        "        for s, p in zip(states, probs):\n",
        "            nextStates[s] = np.prod(p)\n",
        "\n",
        "        return nextStates\n",
        "\n",
        "    def previousStates(self,state, action):\n",
        "      previousStates = {}\n",
        "      for s in self.states:\n",
        "        # Estados seguintes (chaves) e suas probabilidades de transição (valores).\n",
        "        ss = self.nextState(s,action)\n",
        "\n",
        "        # Para cada estado seguinte.\n",
        "        for s_ in ss:\n",
        "          if s_ == state:\n",
        "            # Probabilidade de transição de s para s_ (state).\n",
        "            prob = ss[s_]\n",
        "            previousStates[s] = prob\n",
        "      return previousStates\n",
        "\n",
        "    def reward_structure(self, state, action, next_state):\n",
        "        # Recompensas e custos para cada trabalhadora em cada transição do Processo Markoviano.\n",
        "        # Inicialmente, em cada transição, cada trabalhadora não ganhou nem gastou nada.\n",
        "        ganhos = np.zeros(self.n_W)\n",
        "        custos = np.zeros(self.n_W)\n",
        "\n",
        "        # Para cada trabalhadora.\n",
        "        for x in range(self.n_W):\n",
        "          # Se está entregando um serviço.\n",
        "          if state[x+1] > 0:\n",
        "            # Tem um custo.\n",
        "            custos[x] = self.Hxy[x][state[x+1]-1]\n",
        "            # Se termina o serviço nessa transição, ganha por sua entrega.\n",
        "            if next_state[x+1] == 0:\n",
        "              ganhos[x] = self.Gx[state[x+1]-1]\n",
        "\n",
        "        return ganhos, custos\n",
        "\n",
        "    def reward(self,state, action, next_state):\n",
        "        # Recompensas do sistema todo em cada transição.\n",
        "        ganhos, custos = self.reward_structure(state, action, next_state)\n",
        "        return np.sum(ganhos) - np.sum(custos)\n",
        "\n",
        "    def feasible_actions(self):\n",
        "        actions = [0]\n",
        "        if self.state[0] > 0:\n",
        "          for w in range(self.n_W):\n",
        "            if self.state[w+1] == 0:\n",
        "              actions = actions + [w+1]\n",
        "        return actions\n",
        "\n",
        "    def simulateStep(self,state,action):\n",
        "        nextStates = self.nextState(state,action)\n",
        "        nextState = random.choices( list( nextStates.keys() ), weights = list( nextStates.values() ), k=1 )[0]\n",
        "        r = self.reward(state, action, nextState)\n",
        "\n",
        "        return nextState, r\n",
        "\n",
        "    def step(self,action):\n",
        "        self.state, r  = self.simulateStep(self.state,action)\n",
        "        return self.state, r\n",
        "\n",
        "    def render(self):\n",
        "        print(self.state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROGRAMAÇÃO LINEAR - instanciando mdp"
      ],
      "metadata": {
        "id": "QkLjkwpDFCdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gurobipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNnHSEUgFKwk",
        "outputId": "57014ad5-1d7a-4414-cb3c-3dab72460895"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gurobipy\n",
            "  Downloading gurobipy-12.0.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (16 kB)\n",
            "Downloading gurobipy-12.0.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (14.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gurobipy\n",
            "Successfully installed gurobipy-12.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gurobipy as gp\n",
        "from gurobipy import GRB"
      ],
      "metadata": {
        "id": "a1hQ36nvFFC8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_W = 1\n",
        "n_S = 2\n",
        "maxSteps = 100\n",
        "\n",
        "mdp = BikeWorkers_Environment(n_W,n_S)\n",
        "\n",
        "print('Ganho obtido com cada serviço')\n",
        "print(mdp.Gx)\n",
        "\n",
        "print('Custo médio de cada serviço para cada trabalhadora')\n",
        "print(mdp.Hxy/(mdp.Pwxy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK-ScuAdM8Ws",
        "outputId": "b7184f2e-040f-4c67-dd51-13db95de7481"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ganho obtido com cada serviço\n",
            "[1.42668697 1.06373945]\n",
            "Custo médio de cada serviço para cada trabalhadora\n",
            "[[0.1869272 0.8509917]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.8"
      ],
      "metadata": {
        "id": "kzXzFnSuGHXt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROGRAMAÇÃO LINEAR PRIMAL"
      ],
      "metadata": {
        "id": "JVMPNOikqFJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando modelo.\n",
        "model = gp.Model()"
      ],
      "metadata": {
        "id": "AAjK8rtyFbfc"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variáveis de decisão: função valor dos estados.\n",
        "V = model.addVars(mdp.states, vtype=GRB.CONTINUOUS, name=\"V\", lb=-GRB.INFINITY)"
      ],
      "metadata": {
        "id": "aM7dtARhFyqQ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''model.setObjective(\n",
        "    gp.quicksum(\n",
        "        V[j] for j in mdp.states\n",
        "    ),\n",
        "    GRB.MINIMIZE\n",
        ")'''"
      ],
      "metadata": {
        "id": "hD8P10ZsGXIL"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.setObjective(\n",
        "    V[(0,0)],\n",
        "    GRB.MINIMIZE\n",
        ")"
      ],
      "metadata": {
        "id": "1BLqp01Pfdhj"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzGX8Zw6JFbS",
        "outputId": "897de122-8800-4f31-cf62-09ffb799ea0e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0, 0): <gurobi.Var *Awaiting Model Update*>,\n",
              " (0, 1): <gurobi.Var *Awaiting Model Update*>,\n",
              " (0, 2): <gurobi.Var *Awaiting Model Update*>,\n",
              " (1, 0): <gurobi.Var *Awaiting Model Update*>,\n",
              " (1, 1): <gurobi.Var *Awaiting Model Update*>,\n",
              " (1, 2): <gurobi.Var *Awaiting Model Update*>,\n",
              " (2, 0): <gurobi.Var *Awaiting Model Update*>,\n",
              " (2, 1): <gurobi.Var *Awaiting Model Update*>,\n",
              " (2, 2): <gurobi.Var *Awaiting Model Update*>}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in mdp.states:\n",
        "  for a in mdp.actions:\n",
        "    r = 0\n",
        "    Transitions = mdp.nextState(s,a)\n",
        "    for ss in Transitions:\n",
        "      r = r + Transitions[ss]*(mdp.reward(s,a,ss))\n",
        "\n",
        "    model.addConstr(V[s] - gamma * gp.quicksum(Transitions[ss]*V[ss] for ss in Transitions) >= r, name=f\"func_valor\")"
      ],
      "metadata": {
        "id": "AFrSh9oFGu9h"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in model.getConstrs():\n",
        "    print(f\"{c.ConstrName}: {model.getRow(c)}\")\n",
        "    print(f\"{c.ConstrName}: {c.RHS}\")"
      ],
      "metadata": {
        "id": "sA-84PnGPjaW"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.optimize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OereSaH1MoXT",
        "outputId": "a7fb51d6-32a6-4b6d-d396-239422888b2f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (linux64 - \"Ubuntu 22.04.4 LTS\")\n",
            "\n",
            "CPU model: AMD EPYC 7B12, instruction set [SSE2|AVX|AVX2]\n",
            "Thread count: 1 physical cores, 2 logical processors, using up to 2 threads\n",
            "\n",
            "Optimize a model with 18 rows, 9 columns and 92 nonzeros\n",
            "Model fingerprint: 0x4568e9ea\n",
            "Coefficient statistics:\n",
            "  Matrix range     [1e-03, 1e+00]\n",
            "  Objective range  [1e+00, 1e+00]\n",
            "  Bounds range     [0e+00, 0e+00]\n",
            "  RHS range        [1e-01, 1e+00]\n",
            "Presolve removed 16 rows and 7 columns\n",
            "Presolve time: 0.01s\n",
            "Presolved: 2 rows, 2 columns, 4 nonzeros\n",
            "\n",
            "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
            "       0    0.0000000e+00   1.133818e+00   0.000000e+00      0s\n",
            "       1    5.7330828e-01   0.000000e+00   0.000000e+00      0s\n",
            "\n",
            "Solved in 1 iterations and 0.01 seconds (0.00 work units)\n",
            "Optimal objective  5.733082821e-01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for v in model.getVars():\n",
        "    print(f\"{v.VarName}: {v.X}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XszOtn2gM7JB",
        "outputId": "95523b2e-342e-48fb-d6cb-0cd6c1f7402e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V[0,0]: 0.5733082821474575\n",
            "V[0,1]: 1.7933478991811749\n",
            "V[0,2]: 0.705692829948757\n",
            "V[1,0]: 1.4346783193449402\n",
            "V[1,1]: 1.7933478991811749\n",
            "V[1,2]: 0.7056928299487568\n",
            "V[2,0]: 0.5733082821474575\n",
            "V[2,1]: 1.7933478991811747\n",
            "V[2,2]: 0.7056928299487569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Programação Linear Dual"
      ],
      "metadata": {
        "id": "htXPiMwHqU0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando modelo.\n",
        "model_dual = gp.Model()"
      ],
      "metadata": {
        "id": "h8RQV1jaqfza"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variáveis de decisão: função valor dos estados.\n",
        "x = {}\n",
        "for s in mdp.states:\n",
        "  x[s] = {}\n",
        "  for a in mdp.actions:\n",
        "    x[s][a] = model_dual.addVar(vtype=GRB.CONTINUOUS, name=\"x\", lb=0)"
      ],
      "metadata": {
        "id": "jMoXir8Bqnxn"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR4jxxfwxlzX",
        "outputId": "074bf332-fe8d-4fac-bbe7-b59cca112104"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0, 0): {0: <gurobi.Var *Awaiting Model Update*>,\n",
              "  1: <gurobi.Var *Awaiting Model Update*>},\n",
              " (0, 1): {0: <gurobi.Var *Awaiting Model Update*>,\n",
              "  1: <gurobi.Var *Awaiting Model Update*>},\n",
              " (0, 2): {0: <gurobi.Var *Awaiting Model Update*>,\n",
              "  1: <gurobi.Var *Awaiting Model Update*>},\n",
              " (1, 0): {0: <gurobi.Var *Awaiting Model Update*>,\n",
              "  1: <gurobi.Var *Awaiting Model Update*>},\n",
              " (1, 1): {0: <gurobi.Var *Awaiting Model Update*>,\n",
              "  1: <gurobi.Var *Awaiting Model Update*>},\n",
              " (1, 2): {0: <gurobi.Var *Awaiting Model Update*>,\n",
              "  1: <gurobi.Var *Awaiting Model Update*>},\n",
              " (2, 0): {0: <gurobi.Var *Awaiting Model Update*>,\n",
              "  1: <gurobi.Var *Awaiting Model Update*>},\n",
              " (2, 1): {0: <gurobi.Var *Awaiting Model Update*>,\n",
              "  1: <gurobi.Var *Awaiting Model Update*>},\n",
              " (2, 2): {0: <gurobi.Var *Awaiting Model Update*>,\n",
              "  1: <gurobi.Var *Awaiting Model Update*>}}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função objetivo.\n",
        "model_dual.setObjective(\n",
        "    gp.quicksum(\n",
        "        x[s][a] * sum(\n",
        "            mdp.reward(s, a, ss) * p_transicao\n",
        "            for ss, p_transicao in mdp.nextState(s, a).items()\n",
        "        )\n",
        "        for s in mdp.states\n",
        "        for a in mdp.actions\n",
        "    ),\n",
        "    GRB.MAXIMIZE\n",
        ")"
      ],
      "metadata": {
        "id": "zh2wZi95uL7M"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restrição.\n",
        "for s in mdp.states:\n",
        "  if s == (0,0):\n",
        "    model_dual.addConstr(gp.quicksum(x[s][a] - gamma * gp.quicksum(mdp.previousStates(s,a)[ss] * x[ss][a] for ss in mdp.previousStates(s,a)) for a in mdp.actions) == 1, name=f\"restricao_{s}\")\n",
        "  else:\n",
        "    model_dual.addConstr(gp.quicksum(x[s][a] - gamma * gp.quicksum(mdp.previousStates(s,a)[ss] * x[ss][a] for ss in mdp.previousStates(s,a)) for a in mdp.actions) == 0, name=f\"restricao_{s}\")"
      ],
      "metadata": {
        "id": "Mzpvi1QWz32R"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in model_dual.getConstrs():\n",
        "    print(f\"{c.ConstrName}: {model_dual.getRow(c)}\")\n",
        "    print(f\"{c.ConstrName}: {c.RHS}\")"
      ],
      "metadata": {
        "id": "eu-rxF5c7Qk-"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dual.optimize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a33X4Mw96NUJ",
        "outputId": "81a2f253-d1d3-44e6-c64f-26b24b79f72b"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (linux64 - \"Ubuntu 22.04.4 LTS\")\n",
            "\n",
            "CPU model: AMD EPYC 7B12, instruction set [SSE2|AVX|AVX2]\n",
            "Thread count: 1 physical cores, 2 logical processors, using up to 2 threads\n",
            "\n",
            "Optimize a model with 9 rows, 18 columns and 92 nonzeros\n",
            "Model fingerprint: 0x636180dd\n",
            "Coefficient statistics:\n",
            "  Matrix range     [1e-03, 1e+00]\n",
            "  Objective range  [1e-01, 1e+00]\n",
            "  Bounds range     [0e+00, 0e+00]\n",
            "  RHS range        [1e+00, 1e+00]\n",
            "Presolve removed 1 rows and 8 columns\n",
            "Presolve time: 0.00s\n",
            "Presolved: 8 rows, 10 columns, 42 nonzeros\n",
            "\n",
            "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
            "       0    3.9321272e+30   3.971114e+30   3.932127e+00      0s\n",
            "       7    5.7330828e-01   0.000000e+00   0.000000e+00      0s\n",
            "\n",
            "Solved in 7 iterations and 0.01 seconds (0.00 work units)\n",
            "Optimal objective  5.733082821e-01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQQpbs5LNTgz",
        "outputId": "8cfddedc-2ce7-4ace-f2dd-f2de4a35e18a"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0, 0): {0: <gurobi.Var x (value 0.0)>,\n",
              "  1: <gurobi.Var x (value 3.8097117597918464)>},\n",
              " (0, 1): {0: <gurobi.Var x (value 0.3902882402081556)>,\n",
              "  1: <gurobi.Var x (value 0.0)>},\n",
              " (0, 2): {0: <gurobi.Var x (value 0.0)>, 1: <gurobi.Var x (value 0.0)>},\n",
              " (1, 0): {0: <gurobi.Var x (value 0.0)>,\n",
              "  1: <gurobi.Var x (value 0.584400051942508)>},\n",
              " (1, 1): {0: <gurobi.Var x (value 0.0)>,\n",
              "  1: <gurobi.Var x (value 0.08117717664643773)>},\n",
              " (1, 2): {0: <gurobi.Var x (value 0.0)>, 1: <gurobi.Var x (value 0.0)>},\n",
              " (2, 0): {0: <gurobi.Var x (value 0.11802788800545351)>,\n",
              "  1: <gurobi.Var x (value 0.0)>},\n",
              " (2, 1): {0: <gurobi.Var x (value 0.0)>,\n",
              "  1: <gurobi.Var x (value 0.016394883405601135)>},\n",
              " (2, 2): {0: <gurobi.Var x (value 0.0)>, 1: <gurobi.Var x (value 0.0)>}}"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Execution 1: Manual Control\n",
        "\n",
        "Expand the cell, before executing it."
      ],
      "metadata": {
        "id": "JXy7MjujTN7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def Simulate(env,maxSteps):\n",
        "  # Simula o ambiente até atingir um máximo de passos, passado via parâmetro.\n",
        "  env.reset()\n",
        "  tempo = .25\n",
        "  clear_output(wait=True)\n",
        "  env.render()\n",
        "\n",
        "  steps = 0\n",
        "  eval = 0\n",
        "\n",
        "  while steps < maxSteps:\n",
        "      # Para cada passo.\n",
        "\n",
        "      # Obtém as ações possíveis.\n",
        "      actions = env.feasible_actions()\n",
        "\n",
        "      # Se há ações possíveis além do \"refuse\".\n",
        "      if len(actions) > 1:\n",
        "        # Pede para atribuir o serviço à uma entregadora.              ?\n",
        "        command = input(f\"{steps}({eval}) Choose a worker (\" + str(env.feasible_actions()) + \") to move: \")\n",
        "        for a in command:\n",
        "          # Obtém o estado atual do ambiente.\n",
        "          s = env.state\n",
        "\n",
        "          # Entregadora que receberá o serviço.\n",
        "          a = int(a)\n",
        "          # Para cada ação possível nesse passo.\n",
        "          if a in env.actions:\n",
        "              # Dá o passo e o avalia.\n",
        "              _ , r = env.step(a)\n",
        "              eval = eval + r\n",
        "              steps += 1\n",
        "\n",
        "          clear_output(wait=True)\n",
        "          env.render()\n",
        "          time.sleep(tempo)\n",
        "          break\n",
        "      else:\n",
        "        a = actions[0]\n",
        "\n",
        "        _ , r = env.step(a)\n",
        "        eval = eval + r\n",
        "\n",
        "        steps += 1\n",
        "        clear_output(wait=True)\n",
        "        env.render()\n",
        "        time.sleep(tempo)\n",
        "\n",
        "  print(f\"Congratulations. You earn {eval}.\")\n",
        "\n",
        "\n",
        "  return steps"
      ],
      "metadata": {
        "id": "PUDJ24WK0NV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_W = 3\n",
        "n_S = 2\n",
        "maxSteps = 30\n",
        "\n",
        "env = BikeWorkers_Environment(n_W,n_S)\n",
        "evalGrid = Simulate(env,maxSteps)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wcEOMGdrOrTz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "1081986e-1d10-4831-eeea-6e00951e7df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 1, 0, 0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-adb344e8d9bd>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBikeWorkers_Environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_W\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_S\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mevalGrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxSteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-4fab6538fc61>\u001b[0m in \u001b[0;36mSimulate\u001b[0;34m(env, maxSteps)\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Pede para atribuir o serviço à uma entregadora.              ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{steps}({eval}) Choose a worker (\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeasible_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\") to move: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m           \u001b[0;31m# Obtém o estado atual do ambiente.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution 2: Policies\n",
        "Expand the cells, before executing them."
      ],
      "metadata": {
        "id": "TuxfFuY-efPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def EvaluatePolicyMC(env,policy,maxSteps,nSamples):\n",
        "  # Monte Carlo.\n",
        "\n",
        "  memEval = []\n",
        "\n",
        "  # Para cada teste da política.\n",
        "  for i in range(nSamples):\n",
        "    env.reset()\n",
        "    steps = 0\n",
        "    eval = 0\n",
        "\n",
        "    # Há uma quantidade máxima de passos por teste. Se não, rodaria infinitamente.\n",
        "    while steps < maxSteps:\n",
        "      # Estado do ambiente.\n",
        "      s = env.state\n",
        "      # Ação que a política define que deve ser tomada.\n",
        "      a = policy[s]\n",
        "      # Toma a ação e receve a recompensa por ela.\n",
        "      _, r = env.step(a)\n",
        "      # Atualiza a recompensa.\n",
        "      eval = eval + r\n",
        "      steps += 1\n",
        "\n",
        "    # Soma da recompensa de todas as simulações.\n",
        "    memEval = memEval + [eval]\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    print(f'Simulating {(i+1)/nSamples}')\n",
        "\n",
        "    # Média das recompensas.\n",
        "    print(f'The average gain in simulations was {np.mean(memEval)}.')\n",
        "\n",
        "  return memEval\n",
        "\n",
        "def SimulatePolicy(env,policy,maxSteps):\n",
        "  # Simulação de uma política.\n",
        "  env.reset()\n",
        "  tempo = .5\n",
        "  clear_output(wait=True)\n",
        "  env.render()\n",
        "\n",
        "  eval = 0\n",
        "  steps = 0\n",
        "\n",
        "  # Há quantidade limite de passos.\n",
        "  while steps < maxSteps:\n",
        "    # Estado atual.\n",
        "    s = env.state\n",
        "    # Ação que a política define que deve ser tomada.\n",
        "    a = policy[s]\n",
        "\n",
        "    # Toma a ação.\n",
        "    _, r = env.step(a)\n",
        "    # Atualiza a recompensa.\n",
        "    eval = eval + r\n",
        "    steps += 1\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    print(f'Steps: {steps}')\n",
        "    env.render()\n",
        "    time.sleep(tempo)\n",
        "\n",
        "  # Recompensa gerada pela política após todos os passos.\n",
        "  print(f\"Congratulations. You earn {eval}.\")\n",
        "\n",
        "  return eval\n",
        "\n"
      ],
      "metadata": {
        "id": "pAfI5L1PrZkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cria o ambiente que será utilizado nas simulações."
      ],
      "metadata": {
        "id": "8NP4_Z-T1if0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_W = 3\n",
        "n_S = 2\n",
        "maxSteps = 100\n",
        "\n",
        "env = BikeWorkers_Environment(n_W,n_S)\n",
        "\n",
        "print('Ganho obtido com cada serviço')\n",
        "print(env.Gx)\n",
        "\n",
        "print('Custo médio de cada serviço para cada trabalhadora')\n",
        "print(env.Hxy/(env.Pwxy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R61BSAqV1g8I",
        "outputId": "e9667610-fd6f-44cf-eb03-933531e9b676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ganho obtido com cada serviço\n",
            "[1.26358147 0.86106335]\n",
            "Custo médio de cada serviço para cada trabalhadora\n",
            "[[1.54027377 5.01511069]\n",
            " [0.29335982 0.84023723]\n",
            " [1.61116599 0.20374301]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define by trial and error a policy by setting the variable **policy**.\n"
      ],
      "metadata": {
        "id": "EKzdrd8Bgjp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# policy = {\n",
        "#           (0,0,0,0): 0, (0,0,0,1): 0, (0,0,0,2): 0, (0,0,1,0): 0, (0,0,1,1): 0, (0,0,1,2): 0, (0,0,2,0): 0, (0,0,2,1): 0, (0,0,2,2): 0,\n",
        "#           (0,1,0,0): 0, (0,1,0,1): 0, (0,1,0,2): 0, (0,1,1,0): 0, (0,1,1,1): 0, (0,1,1,2): 0, (0,1,2,0): 0, (0,1,2,1): 0, (0,1,2,2): 0,\n",
        "#           (0,2,0,0): 0, (0,2,0,1): 0, (0,2,0,2): 0, (0,2,1,0): 0, (0,2,1,1): 0, (0,2,1,2): 0, (0,2,2,0): 0, (0,2,2,1): 0, (0,2,2,2): 0,\n",
        "#           (1,0,0,0): 0, (1,0,0,1): 0, (1,0,0,2): 0, (1,0,1,0): 0, (1,0,1,1): 0, (1,0,1,2): 0, (1,0,2,0): 0, (1,0,2,1): 0, (1,0,2,2): 0,\n",
        "#           (1,1,0,0): 0, (1,1,0,1): 0, (1,1,0,2): 0, (1,1,1,0): 0, (1,1,1,1): 0, (1,1,1,2): 0, (1,1,2,0): 0, (1,1,2,1): 0, (1,1,2,2): 0,\n",
        "#           (1,2,0,0): 0, (1,2,0,1): 0, (1,2,0,2): 0, (1,2,1,0): 0, (1,2,1,1): 0, (1,2,1,2): 0, (1,2,2,0): 0, (1,2,2,1): 0, (1,2,2,2): 0,\n",
        "#           (2,0,0,0): 0, (2,0,0,1): 0, (2,0,0,2): 0, (2,0,1,0): 0, (2,0,1,1): 0, (2,0,1,2): 0, (2,0,2,0): 0, (2,0,2,1): 0, (2,0,2,2): 0,\n",
        "#           (2,1,0,0): 0, (2,1,0,1): 0, (2,1,0,2): 0, (2,1,1,0): 0, (2,1,1,1): 0, (2,1,1,2): 0, (2,1,2,0): 0, (2,1,2,1): 0, (2,1,2,2): 0,\n",
        "#           (2,2,0,0): 0, (2,2,0,1): 0, (2,2,0,2): 0, (2,2,1,0): 0, (2,2,1,1): 0, (2,2,1,2): 0, (2,2,2,0): 0, (2,2,2,1): 0, (2,2,2,2): 0,\n",
        "#          }\n",
        "\n",
        "\n",
        "\n",
        "# política que atribui sempre a melhor trabalhadora para um serviço\n",
        "MeanCost = env.Hxy/(env.Pwxy)\n",
        "\n",
        "policy = {}\n",
        "for s in env.states:\n",
        "  if s[0] == 0:\n",
        "    policy[s] = 0\n",
        "  else:\n",
        "    if min(MeanCost[:,s[0]-1]) < env.Gx[s[0]-1]:\n",
        "      policy[s] = np.argmin(MeanCost[:,s[0]-1])+1\n",
        "    else:\n",
        "      policy[s] = 0\n",
        "\n",
        "\n",
        "eval = SimulatePolicy(env, policy, maxSteps)"
      ],
      "metadata": {
        "id": "SUbKeUu2ej9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a6ef44d-7da6-4dff-993b-5c470afab06f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps: 100\n",
            "(0, 0, 0, 0)\n",
            "Congratulations. You earn 17.555765440764052.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the previously defined policy with Monte Carlo simulation."
      ],
      "metadata": {
        "id": "zti388gigzJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxSteps = 100\n",
        "nSamples = 1000\n",
        "e = EvaluatePolicyMC(env, policy, maxSteps, nSamples);"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mSKB6BJJmdjr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7850a2c2-6095-4460-db5b-57089c7db5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulating 1.0\n",
            "The average gain in simulations was 14.91050737181926.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution 3: Analytically Policy Evaluation\n",
        "\n",
        "Expand the cells, before executing them."
      ],
      "metadata": {
        "id": "gzZHndL8baFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def policy_evaluation(Environment,pi, gamma, epsilon):\n",
        "    # Avaliação analítica.\n",
        "    V = {}\n",
        "    # Dicionário com todos os estados possíveis, inicializados em 0.\n",
        "    for s in Environment.states:\n",
        "        V[s] = 0\n",
        "\n",
        "    res = float('inf')\n",
        "\n",
        "    resolution = 10;\n",
        "\n",
        "    steps = 0\n",
        "\n",
        "    while res > epsilon:\n",
        "        V_old = V.copy()\n",
        "        res = 0\n",
        "\n",
        "        # Para cada estado possível.\n",
        "        for s in Environment.states:\n",
        "            V[s] = 0\n",
        "            # Encontra os próximos estados possíveis e suas probabilides.\n",
        "            Transitions = Environment.nextState(s,pi[s])\n",
        "            # Para cada um dos novos estados possíveis, atualiza a recomensa possível a partir do estado.\n",
        "            for ss in Transitions:\n",
        "                V[s] = V[s] + Transitions[ss]*(Environment.reward(s,pi[s],ss) + gamma*V_old[ss])\n",
        "\n",
        "            if abs(V[s] - V_old[s]) > res:\n",
        "                res = abs(V[s] - V_old[s])\n",
        "\n",
        "\n",
        "        steps += 1\n",
        "        if steps % resolution == 0:\n",
        "            clear_output(wait=True)\n",
        "            print(f'Residual {res}')\n",
        "\n",
        "    return V[Environment.state], V\n",
        "\n",
        "def drawValues(env,V):\n",
        "    img = np.zeros((env.Y,env.X))\n",
        "    for j in range(1,env.Y+1):\n",
        "      for i in range(1,env.X+1):\n",
        "        img[env.Y - j,i-1] = V[(i,j)]\n",
        "    plt.imshow(img, cmap='gray', vmin=min(V.values()), vmax=max(V.values()))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pCCk3dskbWvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.99\n",
        "epsilon = 0.0001\n",
        "\n",
        "# política que atribui sempre a melhor trabalhadora para um serviço\n",
        "MeanCost = env.Hxy/(env.Pwxy)\n",
        "\n",
        "policy = {}\n",
        "for s in env.states:\n",
        "  if s[0] == 0:\n",
        "    policy[s] = 0\n",
        "  else:\n",
        "    if min(MeanCost[:,s[0]-1]) < env.Gx[s[0]-1]:\n",
        "      policy[s] = np.argmin(MeanCost[:,s[0]-1])+1\n",
        "    else:\n",
        "      policy[s] = 0\n",
        "\n",
        "eval, V = policy_evaluation(env,policy, gamma, epsilon)\n",
        "\n",
        "# drawValues(env,V)\n",
        "print(f'The expected cumulative reward is {eval}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqUIg_FHbgmV",
        "outputId": "64bffa11-01a0-4c55-d694-02df74d7002e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Residual 9.960497842698146e-05\n",
            "The expected cumulative reward is 15.425177870051483.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution 4: Policy Iteration\n",
        "\n",
        "Expand the cells before executing."
      ],
      "metadata": {
        "id": "loEnmBeycsTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def policy_evaluation(Environment,pi, gamma, epsilon):\n",
        "    # Dicionário com todos os estados possíveis, inicializados em 0.\n",
        "    V = {}\n",
        "    for s in Environment.states:\n",
        "        V[s] = 0\n",
        "\n",
        "    res = float('inf')\n",
        "\n",
        "    while res > epsilon:\n",
        "        V_old = V.copy()\n",
        "        res = 0\n",
        "\n",
        "        # Para cada estado.\n",
        "        for s in Environment.states:\n",
        "            V[s] = 0\n",
        "            # Próximos estados possíveis a partir do estado analisado, e suas probabilidades.\n",
        "            Transitions = Environment.nextState(s,pi[s])\n",
        "            # Para cada um desses possíveis estados futuros, atualiza a recompensa.\n",
        "            for ss in Transitions:\n",
        "                V[s] = V[s] + Transitions[ss]*(Environment.reward(s,pi[s],ss) + gamma*V_old[ss])\n",
        "\n",
        "            if abs(V[s] - V_old[s]) > res:\n",
        "                res = abs(V[s] - V_old[s])\n",
        "\n",
        "    return V\n",
        "\n",
        "def policy_improvement(Environment,V, gamma):\n",
        "    # Política.\n",
        "    pi = {}\n",
        "    # Para cada estado do ambiente.\n",
        "    for s in Environment.states:\n",
        "        # Dicionário para indicar a Função de Valor para cada ação possível, a partir do estado s.\n",
        "        Q = {}\n",
        "        # Para cada ação.\n",
        "        for a in Environment.actions:\n",
        "            Q[a] = 0\n",
        "            # Próximos estados a partir do estado atual e da ação escolhida.\n",
        "            Transitions = Environment.nextState(s,a)\n",
        "            # Atualiza a Função de Valor.\n",
        "            for ss in Transitions:\n",
        "                Q[a] = Q[a] + Transitions[ss]*(Environment.reward(s,a,ss) + gamma*V[ss])\n",
        "\n",
        "        # A política do ambiente para o estado s será tomar a ação que maximiza a Função de Valor.\n",
        "        pi[s] = max(Q, key=Q.get)\n",
        "    return pi\n",
        "\n",
        "def policy_iteration(Environment, gamma, eps):\n",
        "\n",
        "    res = 10\n",
        "\n",
        "    # Cria uma política inicial (sempre indica \"refuse\").\n",
        "    policy = {}\n",
        "    for s in Environment.states:\n",
        "      policy[s] = 0;\n",
        "\n",
        "    # Avalia a política criada.\n",
        "    V = policy_evaluation(Environment, policy, gamma, eps)\n",
        "\n",
        "\n",
        "    # renderPolicy(Environment,policy)\n",
        "\n",
        "    # input(f'O valor da política é {V[(1,1)]}. Pressione enter para continuar.')\n",
        "\n",
        "    while res > eps:\n",
        "        # Melhora a política.\n",
        "        policy = policy_improvement(Environment,V, gamma)\n",
        "        V_old = V.copy()\n",
        "        # Avalia a nova política.\n",
        "        V = policy_evaluation(Environment, policy, gamma, eps)\n",
        "\n",
        "        res = np.max(  np.abs( np.array(list(V.values())) -  np.array(list(V_old.values())) ) )\n",
        "\n",
        "        print(f'Residual {res}')\n",
        "\n",
        "        # clear_output(wait=True)\n",
        "        # renderPolicy(Environment,policy)\n",
        "\n",
        "        print(f'O valor da política é {V[(0,0,0,0)]}.')\n",
        "\n",
        "    return policy\n",
        "\n"
      ],
      "metadata": {
        "id": "9a0H2ftActGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.99\n",
        "eps = 0.0001\n",
        "\n",
        "policy_iteration(env,gamma,eps);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnHvfMXccycs",
        "outputId": "77007dd5-12d7-4ad8-88dc-7664c3b97238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Residual 1.3024656696020485\n",
            "O valor da política é 1.1017324591747242.\n",
            "Residual 0.0\n",
            "O valor da política é 1.1017324591747242.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution 5: Value Iteration\n",
        "\n",
        "Expand the cells before executing."
      ],
      "metadata": {
        "id": "OfAJsH1ffsXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(Environment,gamma, eps):\n",
        "    # Iteração de Valor, para encontrar uma política ótima.\n",
        "\n",
        "    # Inicializa a função de valor, inicialmente com 0 para todos os estados.\n",
        "    V = {}\n",
        "    for s in Environment.states:\n",
        "        V[s] = 0\n",
        "\n",
        "    res = float('inf')\n",
        "\n",
        "    # Definir epsilon (precisão desejada), conforme gamma (fator de desconto).                     ?\n",
        "    if gamma < 1:\n",
        "        epsilon = eps*(1-gamma)/(2*gamma)\n",
        "    else:\n",
        "        epsilon = eps\n",
        "\n",
        "    while res > epsilon:\n",
        "        V_old = V.copy()\n",
        "        res = 0\n",
        "\n",
        "#       Encontra a função valor epsilon-ótima\n",
        "        # Para cada estado.\n",
        "        for s in Environment.states:\n",
        "            Q = {}\n",
        "            # Testa a Função de Valor para todas as ações.\n",
        "            for a in Environment.actions:\n",
        "                Q[a] = 0\n",
        "                Transitions = Environment.nextState(s,a)\n",
        "                for ss in Transitions:\n",
        "                    Q[a] = Q[a] + Transitions[ss]*(Environment.reward(s,a,ss) + gamma*V_old[ss])\n",
        "\n",
        "            # A função valor do estado s é o maior valor obtido no teste das ações possíveis.\n",
        "            V[s] = max(Q.values())\n",
        "\n",
        "            if abs(V[s] - V_old[s]) > res:\n",
        "                res = abs(V[s] - V_old[s])\n",
        "\n",
        "        print(f'Residual {res}')\n",
        "\n",
        "        clear_output(wait=True)\n",
        "\n",
        "#   Extrai a política epsilon-ótima\n",
        "    pi = {}\n",
        "    for s in Environment.states:\n",
        "        Q = {}\n",
        "        for a in Environment.actions:\n",
        "            Q[a] = 0\n",
        "            Transitions = Environment.nextState(s,a)\n",
        "            for ss in Transitions:\n",
        "                Q[a] = Q[a] + Transitions[ss]*(Environment.reward(s,a,ss) + gamma*V[ss])\n",
        "\n",
        "        pi[s] = max(Q, key=Q.get)\n",
        "\n",
        "\n",
        "    return pi, V"
      ],
      "metadata": {
        "id": "zFUNWgCmf0Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.99\n",
        "epsilon = 0.01\n",
        "\n",
        "\n",
        "policy, eval = value_iteration(env,gamma, epsilon)\n",
        "# renderPolicy(env,policy)\n",
        "print(f'O valor da política é {eval}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JSMJAmYgX4H",
        "outputId": "5855a044-3aac-468d-a70a-6ac886eef2ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Residual 9.186989371512411e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Ganho obtido com cada serviço')\n",
        "print(env.Gx)\n",
        "\n",
        "print('Custo médio de cada serviço para cada trabalhadora')\n",
        "print(env.Hxy/(env.Pwxy))\n",
        "\n",
        "print('Tempo médio de cada serviço para cada trabalhadora')\n",
        "print(1/(env.Pwxy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGykw9_0jBnB",
        "outputId": "dce5e69c-0c96-439b-ab52-5c3c7548815f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ganho obtido com cada serviço\n",
            "[1.20471269 1.27851004]\n",
            "Custo médio de cada serviço para cada trabalhadora\n",
            "[[0.36460998 0.84975446]\n",
            " [0.36307743 1.34929224]\n",
            " [6.57762171 0.46000512]]\n",
            "Tempo médio de cada serviço para cada trabalhadora\n",
            "[[1.21760547 1.39190045]\n",
            " [2.22895737 2.051881  ]\n",
            " [7.96152948 1.10675432]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBSH75WZixom",
        "outputId": "e3655cad-6610-4e0f-ba09-e9974fb147fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0, 0, 0, 0): 0,\n",
              " (0, 0, 0, 1): 0,\n",
              " (0, 0, 0, 2): 0,\n",
              " (0, 0, 1, 0): 0,\n",
              " (0, 0, 1, 1): 0,\n",
              " (0, 0, 1, 2): 0,\n",
              " (0, 0, 2, 0): 0,\n",
              " (0, 0, 2, 1): 0,\n",
              " (0, 0, 2, 2): 0,\n",
              " (0, 1, 0, 0): 0,\n",
              " (0, 1, 0, 1): 0,\n",
              " (0, 1, 0, 2): 0,\n",
              " (0, 1, 1, 0): 0,\n",
              " (0, 1, 1, 1): 0,\n",
              " (0, 1, 1, 2): 0,\n",
              " (0, 1, 2, 0): 0,\n",
              " (0, 1, 2, 1): 0,\n",
              " (0, 1, 2, 2): 0,\n",
              " (0, 2, 0, 0): 0,\n",
              " (0, 2, 0, 1): 0,\n",
              " (0, 2, 0, 2): 0,\n",
              " (0, 2, 1, 0): 0,\n",
              " (0, 2, 1, 1): 0,\n",
              " (0, 2, 1, 2): 0,\n",
              " (0, 2, 2, 0): 0,\n",
              " (0, 2, 2, 1): 0,\n",
              " (0, 2, 2, 2): 0,\n",
              " (1, 0, 0, 0): 1,\n",
              " (1, 0, 0, 1): 2,\n",
              " (1, 0, 0, 2): 1,\n",
              " (1, 0, 1, 0): 1,\n",
              " (1, 0, 1, 1): 1,\n",
              " (1, 0, 1, 2): 1,\n",
              " (1, 0, 2, 0): 1,\n",
              " (1, 0, 2, 1): 1,\n",
              " (1, 0, 2, 2): 1,\n",
              " (1, 1, 0, 0): 2,\n",
              " (1, 1, 0, 1): 2,\n",
              " (1, 1, 0, 2): 2,\n",
              " (1, 1, 1, 0): 0,\n",
              " (1, 1, 1, 1): 0,\n",
              " (1, 1, 1, 2): 0,\n",
              " (1, 1, 2, 0): 0,\n",
              " (1, 1, 2, 1): 0,\n",
              " (1, 1, 2, 2): 0,\n",
              " (1, 2, 0, 0): 2,\n",
              " (1, 2, 0, 1): 2,\n",
              " (1, 2, 0, 2): 2,\n",
              " (1, 2, 1, 0): 0,\n",
              " (1, 2, 1, 1): 0,\n",
              " (1, 2, 1, 2): 0,\n",
              " (1, 2, 2, 0): 0,\n",
              " (1, 2, 2, 1): 0,\n",
              " (1, 2, 2, 2): 0,\n",
              " (2, 0, 0, 0): 3,\n",
              " (2, 0, 0, 1): 1,\n",
              " (2, 0, 0, 2): 1,\n",
              " (2, 0, 1, 0): 3,\n",
              " (2, 0, 1, 1): 1,\n",
              " (2, 0, 1, 2): 1,\n",
              " (2, 0, 2, 0): 3,\n",
              " (2, 0, 2, 1): 1,\n",
              " (2, 0, 2, 2): 1,\n",
              " (2, 1, 0, 0): 3,\n",
              " (2, 1, 0, 1): 0,\n",
              " (2, 1, 0, 2): 0,\n",
              " (2, 1, 1, 0): 3,\n",
              " (2, 1, 1, 1): 0,\n",
              " (2, 1, 1, 2): 0,\n",
              " (2, 1, 2, 0): 3,\n",
              " (2, 1, 2, 1): 0,\n",
              " (2, 1, 2, 2): 0,\n",
              " (2, 2, 0, 0): 3,\n",
              " (2, 2, 0, 1): 0,\n",
              " (2, 2, 0, 2): 0,\n",
              " (2, 2, 1, 0): 3,\n",
              " (2, 2, 1, 1): 0,\n",
              " (2, 2, 1, 2): 0,\n",
              " (2, 2, 2, 0): 3,\n",
              " (2, 2, 2, 1): 0,\n",
              " (2, 2, 2, 2): 0}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GsyO9ZFuH1Ps"
      }
    }
  ]
}